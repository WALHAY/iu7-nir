\chapter{Классификация и основные подходы к обнаружению заимствований}

\section{Классификация алгоритмов и методов детекции}

Все многообразие подходов к обнаружению некорректных заимствований можно систематизировать в четыре основных класса:

\begin{enumerate}
	
	\item методы синтаксического сравнения и <<отпечатков>> текста;
	
	\item лексико-статистические методы и метрики схожести;
	
	\item методы семантического/смыслового сопоставления;
	
	\item модели авторского стиля и стилометрия.
	
\end{enumerate}

Такая классификация позволяет проанализировать каждый подход с точки зрения его особенностей, преимуществ и ограничений. Дополнительно рассматриваются алгоритмы анализа и верификации цитирования как специализированное направление детекции некорректных заимствований~\cite{zelenkov2007}.

\chapter{Синтаксические и лексико-статистические методы}

\section{Методы синтаксического сравнения и отпечатков текста}

Для решения задачи обнаружения нечетких дубликатов текстов применяются методы синтаксического сравнения. Идея этих методов заключается в том, чтобы получить компактное представление текста, сохраняющее его уникальные черты, и сравнивать эти представления вместо полных текстов. Такое представление называется <<отпечатком>> или сигнатурой документа. Это позволяет значительно ускорить сравнение больших объемов текстовых данных. Основной принцип состоит в следующем: текст разбивается на перекрывающиеся или неперекрывающиеся фрагменты фиксированной длины, каждому фрагменту вычисляется хеш-значение, и затем сравниваются хеши~\cite{zelenkov2007}.

\subsection{Шинглы и метод Бродера}

Одним из первых исследований в области нахождения нечетких дубликатов является работа А. Бродера, в которой был предложен синтаксический метод оценки сходства между документами, основанный на представлении документа в виде множества всевозможных последовательностей фиксированной длины $k$, состоящих из соседних слов. Такие последовательности были названы шинглами (k-грамм слов). Два документа считались похожими, если их множества шинглов существенно пересекались.

Мера схожести двух документов вычисляется с помощью коэффициента Жаккара:

\[
J(A, B) = \frac{|A \cap B|}{|A \cup B|},
\]

где $A$ и $B$ --- множества шинглов двух документов. Значения коэффициента близкие к 1 указывают на высокую схожесть документов, а близкие к 0 --- на отсутствие схожести.

Поскольку число шинглов примерно равно длине документа в словах, были предложены два метода сэмплирования для получения репрезентативных подмножеств. Первый метод оставлял только те шинглы, чьи дактилограммы (численные отпечатки, вычисляемые по алгоритму Рабина-Карпа) делились без остатка на некоторое число $m$. Второй метод отбирал фиксированное число $s$ шинглов с наименьшими значениями дактилограмм~\cite{zelenkov2007}.

\subsection{Алгоритм Winnowing}

Winnowing --- это модификация простого $k$-граммного анализа, предложенная для повышения эффективности. Алгоритм работает следующим образом: текст разбивается на $k$-граммы (подстроки из $k$ символов); для каждой $k$-граммы вычисляется хеш-значение; из всех хешей выбираются только минимальные значения в пределах скользящего окна размером $w$. Эти избранные хеши образуют отпечаток документа, после чего отпечатки различных документов сравниваются для определения схожести. Преимущество алгоритма Winnowing состоит в устойчивости к перестановкам фрагментов и синтаксическим изменениям, при этом сохраняя низкую вычислительную сложность. Алгоритм используется в системах MOSS и JPlag для обнаружения плагиата в исходном коде и текстах~\cite{matec_winnowing2018}.

\subsection{Дактилограммы и алгоритм Рабина-Карпа}

Одними из первых исследований в области нахождения нечетких дубликатов являются работы U. Manber и N. Heintze. Дактилограмма (также называемая <<отпечатком>> или хеш-сигнатурой) файла или документа включает все текстовые подстроки фиксированной длины. Численное значение дактилограмм вычисляется с помощью алгоритма случайных полиномов Рабина-Карпа. Дактилограмма отличается от простого отпечатка тем, что представляет собой набор множественных хеш-значений для разных подстрок, а не единственное значение. В качестве меры сходства двух документов используется отношение числа общих подстрок к размеру файла или документа. Алгоритм Рабина-Карпа основан на быстром вычислении хешей для перекрывающихся подстрок с использованием скользящего окна. Полиномиальное хеширование позволяет за $O(1)$ пересчитать хеш для следующей подстроки на основе хеша предыдущей. Метод особенно эффективен при поиске точных совпадений в больших массивах текста~\cite{zelenkov2007}.

\subsection{Мегашинглы и супершинглы}

Дальнейшим развитием концепций Бродера являются исследования D. Fetterly. Для каждого документа вычисляются 84 дактилограммы по алгоритму Рабина-Карпа с помощью взаимно-однозначных и независимых функций. В результате каждый документ представлялся 84 шинглами, минимизирующими значение соответствующей функции. Затем 84 шингла разбиваются на 6 групп по 14 шинглов в каждой. Эти группы называются супершинглами. Документ представляется всевозможными попарными сочетаниями из 6 супершинглов, которые называются мегашинглами. Число таких мегашинглов равно 15. Два документа сходны по содержанию, если у них совпадает хотя бы один мегашингл. Ключевое преимущество данного алгоритма состоит в том, что любой документ (в том числе и очень маленький) всегда представляется вектором фиксированной длины, и сходство определяется простым сравнением координат вектора~\cite{zelenkov2007}.

\subsection{N-граммный анализ и SimHash}

$N$-граммный анализ --- одна из самых базовых, но действенных методик. Документ представляется как набор $n$-грамм (последовательности из $n$ символов или слов). Сравнение документов производится по пересечению их $n$-грамм. Алгоритм не учитывает порядок $n$-грамм в документе и работает с ними как с множеством. SimHash расширяет эту идею: для каждого документа строится битовый отпечаток фиксированной длины путем комбинирования хешей его $n$-грамм. Два документа считаются схожими, если расстояние Хемминга между их отпечатками мало~\cite{mdpi_fingerprint2021}.

\subsection{Locality Sensitive Hashing (LSH)}

Locality Sensitive Hashing (LSH) --- это техника быстрого поиска похожих объектов (документов, предложений, наборов слов или векторов признаков) в больших коллекциях. Алгоритм строит такие хеш-функции, что близкие объекты с высокой вероятностью попадают в одну и ту же <<корзину>> (bucket), а далекие --- в разные корзины.

Типичный сценарий при поиске похожих текстов по словам выглядит следующим образом:

\begin{enumerate}
	\item текст каждого документа преобразуется в компактное представление: множество шинглов, вектор численных признаков (например, TF $\cdot$ IDF) или семантический вектор документа.
	\item для этих представлений выбирается семейство локально-чувствительных хеш-функций, максимизирующих вероятность совпадения хешей для близких объектов (по косинусной мере близости или по коэффициенту Жаккара).
	\item хеш-значения группируются в несколько <<полос>> и заносятся в несколько хеш-таблиц. Объекты, имеющие одинаковые хеши в одной и той же полосе, попадают в одну корзину.
	\item при поиске похожего документа или фрагмента текста его представление хешируется тем же набором функций. По полученным хешам находятся все объекты, попавшие в те же корзины во всех таблицах --- это кандидаты на сходство по набору слов или признаков.
	\item только для небольшой подвыборки кандидатов выполняется более точное сравнение (например, по косинусной мере близости или по пересечению шинглов).
\end{enumerate}

За счет такого двухэтапного поиска (быстрый отбор кандидатов на совпадение + точная проверка) LSH позволяет за логарифмическое или даже сублогарифмическое время находить похожие тексты без полного попарного сравнения со всеми документами в базе. Метод особенно ценен для масштабируемых систем с миллионами или миллиардами документов~\cite{mdpi_fingerprint2021}.

\section{Лексико-статистические методы и метрики схожести}

Лексико-статистические методы и метрики схожести представляют собой класс алгоритмов, которые решают задачу выявления заимствований путем анализа статистических характеристик текста на уровне отдельных слов и терминов. В отличие от синтаксических методов, которые работают с символьными последовательностями и локальными структурами, лексико-статистические подходы рассматривают документ как набор терминов (слов) и их частотных характеристик.

Основная идея этого класса методов заключается в том, что два заимствованных или скопированных текста, даже если они переформулированы или переставлены, будут содержать сходный набор ключевых слов и терминов. Метрики схожести позволяют количественно оценить степень совпадения между наборами слов двух документов и определить, насколько они семантически близки.

Для реализации лексико-статистических методов документы преобразуются в компактные представления: либо в виде векторов численных признаков (таких как TF $\cdot$ IDF), либо в виде бинарных сигнатур на основе отобранного подмножества слов. Затем для сравнения этих представлений применяются различные метрики расстояния и схожести: косинусная мера близости, коэффициент Жаккара, расстояние Евклида, расстояние Левенштейна и другие.

\subsection{Численный признак TF $\cdot$ IDF и векторное пространство}

Для решения задачи сравнения документов на основе терминов используются лексико-статистические методы. Одним из наиболее важных методов является представление документа в виде вектора численных признаков. TF $\cdot$ IDF (Term Frequency-Inverse Document Frequency) --- это численный признак, характеризующий важность термина в документе относительно всей коллекции. Величина TF $\cdot$ IDF рассчитывается как произведение двух компонент: частоты термина в документе (TF) и его редкости в коллекции (IDF).

Такое представление документа по словам и их весам часто описывают моделью <<мешка слов>>. В этой модели порядок слов в документе полностью игнорируется: важен только набор терминов и их частоты. Это делает метод устойчивым к перестановкам фраз, но менее чувствительным к изменениям порядка слов, критичным для смысла.

Построение вектора TF $\cdot$ IDF для документа происходит следующим образом: 
\begin{enumerate}
	\item строится частотный словарь документа;
	\item для каждого слова вычисляется произведение TF $\cdot$ IDF;
	\item вектор упорядочивается по убыванию этого произведения;
	\item выбираются топ-$N$ слов с наибольшими весами и сцепляются в алфавитном порядке;
	\item в качестве сигнатуры документа вычисляется контрольная сумма (например, CRC32 или MD5) полученной строки.
\end{enumerate}   

Два текста сравниваются как векторы в многомерном пространстве, где каждое слово представляет одну координату. Схожесть между двумя такими векторами оценивается с помощью косинусной меры близости (косинус угла между векторами). Косинусная мера близости принимает значения от $-1$ до $+1$, где значение 1 означает полное совпадение, 0 означает ортогональность (отсутствие схожести), а $-1$ означает полную противоположность. Документы с высокой косинусной мерой близости (обычно выше порога 0.8) считаются потенциально плагиатными~\cite{tfidf_hybrid2022}.

\subsection{Численный признак TF $\cdot$ RIDF}

Близкой по идее к признаку TF $ \cdot $ IDF является модификация TF $ \cdot $ RIDF (Term Frequency–Residual Inverse Document Frequency), основанная на понятии остаточной обратной документной частоты RIDF. Как показывают Зеленков и Сегалович, RIDF вводится как разность между ``наблюдаемой'' величиной IDF и её теоретическим ожиданием в модели Пуассона распределения слов по документам коллекции~\cite{zelenkov2007}.

Пусть \(N\) --- число документов в коллекции, \(df\) --- число документов, в которых слово встречается хотя бы один раз, а \(cf\) --- суммарная частота данного слова по всей коллекции. Тогда ``обычная'' обратная документная частота вычисляется как
\[
\mathrm{IDF} = -\log\frac{df}{N},
\]
а теоретически ожидаемое количество информации о факте появления слова в документе при пуассоновской модели задаётся выражением
\[
P_{\mathrm{IDF}} = -\log\bigl(1 - \exp(-cf/N)\bigr).
\]
Остаточная IDF (Residual IDF) определяется как
\[
\mathrm{RIDF} = \mathrm{IDF} - P_{\mathrm{IDF}}
= -\log\frac{df}{N} + \log\bigl(1 - \exp(-cf/N)\bigr).
\]

Интерпретация величины \( \mathrm{RIDF} \) состоит в том, что она измеряет прирост информации, содержащейся в реальном распределении слова по документам, по сравнению с равномерным случайным (пуассоновским) распределением. ``Хорошие'', содержательные слова, как правило, распределены неравномерно и встречаются в относительно небольшом числе документов, что даёт большие значения \( \mathrm{RIDF} \); напротив, общеязыковые и малоинформативные слова рассеиваются практически равномерно по всей коллекции и имеют малые значения \( \mathrm{RIDF} \)~\cite{zelenkov2007}.

Практическая схема вычисления признака TF $ \cdot $ RIDF у Зеленкова и Сегаловича выглядит следующим образом. По всей коллекции строится словарь, ставящий каждому слову в соответствие число документов \(df\), в которых оно встречается хотя бы один раз, и суммарную частоту \(cf\) по коллекции. Затем для конкретного документа строится его частотный словарь, и для каждого слова рассчитывается вес
\[
w_t = TF \cdot \mathrm{RIDF},
\qquad
TF = 0.5 + 0.5 \cdot \frac{tf}{tf_{\max}},
\]
где \(tf\) --- частота слова в данном документе, а \(tf_{\max}\) --- максимальная частота какого‑либо слова в этом же документе~\cite{zelenkov2007}.

Как и в случае TF $ \cdot $ IDF, далее выбираются несколько слов (в оригинальной работе --- шесть) с наибольшими значениями \(w_t\), приводятся к канонической форме, упорядочиваются в алфавитном порядке и сцепляются в одну строку; над полученной строкой вычисляется контрольная сумма (например, CRC32), которая и используется как лексическая сигнатура документа~\cite{zelenkov2007}.

\subsection{Метод I-Match}

Для решения задачи быстрого выявления дубликатов применяется сигнатурный подход, основанный на лексических принципах, предложенный A. Chowdhury. Основная идея метода I-Match состоит в вычислении дактилограммы для представления содержания документов на основе отобранного подмножества слов. Сначала для исходной коллекции документов строится словарь $L$, который включает слова со средними значениями IDF (исключаются очень частые служебные слова и очень редкие слова). Для каждого документа формируется множество $U$ различных слов, входящих в него, и определяется пересечение $U$ и словаря $L$. Список слов, входящих в пересечение, упорядочивается, и для него вычисляется I-Match сигнатура (хеш-функция SHA1). Два документа считаются похожими, если у них совпадают I-Match сигнатуры~\cite{zelenkov2007}.

\subsection{Метод опорных слов}

Метод опорных слов, предложенный С. Ильинским, применяется для выявления дубликатов на основе двоичного представления документа. Сначала из индекса по определенному правилу выбирается множество из $N$ слов, называемых опорными. Затем каждый документ представляется $N$-мерным двоичным вектором, где $i$-я координата равна 1, если $i$-е опорное слово имеет в документе относительную частоту выше определенного порога, и равна 0 в противном случае. Этот двоичный вектор называется сигнатурой документа. Два документа похожи, если у них совпадают сигнатуры или совпадает большинство бит. Для каждого слова строится распределение документов по внутридокументной частоте. Проводится несколько итераций оптимизации, в которых максимизируется покрытие документов при фиксированной точности, а затем максимизируется точность при фиксированном покрытии~\cite{zelenkov2007}.

\subsection{Расстояние Левенштейна}

Для решения задачи поиска близких вариантов фраз и обнаружения парафраза применяется редакционное расстояние. Расстояние Левенштейна --- это минимальное количество однозначных операций редактирования (вставка, удаление, замена символа), необходимых для преобразования одной строки в другую. Метрика полезна для поиска близких вариаций текста, включая опечатки и синтаксические ошибки. Однако расстояние Левенштейна не учитывает порядок элементов --- два текста с переставленными предложениями будут считаться различными. На больших текстах требует квадратичного времени вычисления $O(n \cdot m)$, что ограничивает его применимость в масштабных системах~\cite{mit_levenshtein2020}.

\subsection{Сигнатуры на основе отдельных предложений документа}

Исследование Зеленкова и Сегаловича включает алгоритмы, основанные на выборе характерных предложений документа. Эти методы решают задачу быстрого выявления потенциально похожих документов за счет анализа наиболее информативных фрагментов текста.

\textit{Long Sent}: выбираются 2 самых длинных предложения документа, сцепляются в алфавитном порядке, и вычисляется контрольная сумма CRC32. Алгоритм предполагает, что длинные предложения содержат больше информации и менее вероятно копируются с модификациями. Этот алгоритм показал наивысшую $F$-меру (0.82) среди всех исследованных методов.

\textit{Heavy Sent}: вычисляется вес каждого предложения как сумма произведений TF $\cdot$ IDF для всех слов предложения. Выбираются 2 самых тяжелых (информативных) предложения, и для них вычисляется сигнатура. Метод не учитывает порядок предложений в документе~\cite{zelenkov2007}.

\chapter{Семантические методы и стилометрический анализ}

Методы, рассматриваемые в данном разделе, решают задачу семантического сопоставления текстов: необходимо определить, являются ли два фрагмента текста смысловыми эквивалентами, содержат ли они парафраз, тематически близкую информацию или скрытое заимствование. В отличие от синтаксических и лексико-статистических подходов, семантические методы стремятся учитывать не только совпадение слов, но и их значение в контексте, что позволяет выявлять заимствования при глубокой переформулировке исходного текста.

\section{Методы семантического сопоставления}

\subsection{Встраивания слов: Word2Vec, fastText, GloVe}

Для решения задачи выявления парафраза и семантически эквивалентных текстов применяются методы семантического сопоставления на основе встраиваний слов (word embeddings). Встраивание слова --- это отображение каждого слова в плотный вещественный вектор в пространстве низкой размерности, где геометрическая близость векторов соответствует семантической близости слов.

Основная идея метода состоит в следующем: вместо того чтобы сравнивать тексты по совпадению слов, каждый текст переводится в векторное представление, а затем сравниваются уже эти векторы. Тексты с близкими векторами (по косинусной мере близости) считаются семантически похожими, даже если в них используются различные, но синонимичные или близкие по смыслу слова~\cite{arxiv_bertembed2020}.

Классические модели встраиваний слов включают:

\begin{itemize}
	\item Word2Vec --- обучает векторы слов с помощью задач Skip-gram или CBOW, предсказывая слово по контексту или контекст по слову и тем самым кодируя распределительную семантику слов;
	\item fastText --- расширяет Word2Vec за счёт учёта символьных $n$-грамм, что позволяет лучше обрабатывать редкие и морфологически сложные слова;
	\item GloVe --- строит векторы слов на основе матричной факторизации глобальной матрицы совместных встречаемостей слов в корпусе.
\end{itemize}

Для перехода от векторов отдельных слов к вектору всего документа используются различные способы построения представления текста:

\begin{itemize}
	\item простое усреднение векторов всех слов документа;
	\item взвешенное усреднение с весами TF $\cdot$ IDF, подчеркивающее содержательные слова;
	\item специализированные модели представления предложений и документов (Doc2Vec, Sentence-BERT и др.), возвращающие вектор всего фрагмента текста~\cite{aclan_phrase_bert2021, arxiv_lstm_plag2021}.
\end{itemize}

Далее два текста сравниваются как векторы в многомерном пространстве с помощью косинусной меры близости. Высокие значения косинусной меры близости свидетельствуют о семантическом сходстве документов и потенциальном наличии парафраза или смыслового плагиата.

Таким образом, методы на основе встраиваний слов решают задачу семантического сопоставления текстов: они переводят тексты в векторное пространство и позволяют находить скрытые заимствования за счет сравнения смысловых, а не только лексических характеристик~\cite{arxiv_bertembed2020, aclan_phrase_bert2021}.

\subsection{BERT и трансформерные модели}

BERT (Bidirectional Encoder Representations from Transformers) --- модель глубокого обучения, предварительно обученная на большом количестве текста, которая генерирует контекстные представления слов и предложений. В отличие от статических встраиваний, BERT учитывает контекст слова в предложении, что позволяет более точно захватывать смысл. Sentence-BERT (SBERT) расширяет BERT для создания встраиваний всех предложений, которые прямо оптимизированы для семантической схожести. Русскоязычные варианты BERT (например, RuBERT) обеспечивают качественное представление текстов на русском языке~\cite{aclan_phrase_bert2021}.

\subsection{Siamese и Triplet Loss архитектуры}

Данный класс методов решает задачу обучения метрического пространства, в котором расстояние между представлениями текстов отражает степень их семантического сходства. Цель состоит в том, чтобы вектора парафраз и корректных переформулировок располагались близко друг к другу, а несвязанные тексты --- далеко.

Siamese архитектура состоит из двух или более копий одной и той же нейронной сети, которые обрабатывают два входа и генерируют представления, сравниваемые для определения схожести. Triplet loss минимизирует расстояние между якорным примером и похожим примером, одновременно максимизируя расстояние между якорем и непохожим примером. Таким образом, модель явно обучается различать пары <<заимствование\,/\,незаимствование>>.

В качестве базового энкодера в таких архитектурах могут использоваться как статические встраивания слов, так и контекстные модели (BiLSTM, трансформеры). В последнем случае порядок слов и структура предложения учитываются, что особенно важно для детекции сложного парафраза и стилевого заимствования~\cite{pmc_siamese2022}.

\subsection{LSTM и RNN с механизмом внимания}

Recurrent Neural Networks (RNN), особенно в форме LSTM (Long Short-Term Memory) или GRU (Gated Recurrent Unit), решают задачу моделирования последовательности слов в тексте и захвата долгосрочных зависимостей между ними. Это позволяет выявлять случаи заимствования, когда сохраняется общая структура и логика изложения, но отдельные слова и фразы существенно изменены.

Такие сети могут обрабатывать текст пословно или по подсловам и по мере чтения <<запоминать>> важную информацию о ранее встреченных словах. Добавление attention‑механизма позволяет модели сосредоточиться на наиболее релевантных частях входа при сравнении двух текстов, сопоставляя между собой фразы и предложения. BiLSTM (bidirectional LSTM) обрабатывает текст в обоих направлениях, улучшая представление.

Поскольку RNN‑архитектуры явно работают с последовательностью, они учитывают порядок слов и предложений в документе, что критически важно для выявления сложных парафраз и стилистических заимствований~\cite{arxiv_lstm_plag2021}.

\section{Модели авторского стиля и стилометрия}

\subsection{Основы стилометрического анализа}

Стилометрия --- это область, изучающая характеристики письменного стиля, которые отличают одного автора от другого. Предполагается, что у каждого автора есть уникальный стиль, который сохраняется даже при осознанной попытке его изменить. Признаки стилометрии включают:

\begin{itemize}
	
	\item среднюю длину предложения, распределение длин предложений;
	
	\item часто используемые функциональные слова (предлоги, союзы, артикли);
	
	\item частотность части речи (POS tags);
	
	\item лексическое разнообразие (type-token ratio);
	
	\item использование пунктуации и заглавных букв;
	
	\item лексическую плотность;
	
	\item читаемость текста.
	
\end{itemize}

Стилометрический анализ применяется как для авторского сопоставления (определение авторства текста), так и для интринсивной детекции плагиата~\cite{acl_intrinsic_2015}.

\subsection{Интринсивная детекция плагиата}

Интринсивная (intrinsic) детекция плагиата ищет признаки копирования внутри документа, без привлечения внешних источников. Основной метод решает задачу поиска стилистических разрывов: фрагменты заимствованного текста обычно имеют стиль отличающийся от основного стиля документа. Путем анализа последовательных блоков текста можно выявить участки, где стиль резко меняется. Применяются статистические тесты для определения значимости изменения стилистических параметров~\cite{acl_intrinsic_2015}.

\section{Анализ и верификация цитирования}

Некорректное цитирование представляет собой особый вид неправильных заимствований, который включает не только прямое копирование текста без указания источника, но и более сложные формы академической недобросовестности: ошибки в оформлении библиографических ссылок, использование информации без должной атрибуции, искажение смысла первоисточника, отсылки к несуществующим или неверным источникам. Автоматизированная верификация цитирования позволяет выявить эти проблемы и повысить качество научных публикаций.

\subsection{Проблемы и типы ошибок в цитировании}

Ошибки в цитировании принимают различные формы и являются типичными видами некорректных заимствований:

\begin{itemize}
	
	\item некорректный источник --- ссылка приведена неправильно, источник не существует, или указывает на совершенно иное произведение;
	
	\item отсутствующая информация в источнике --- утверждение, приписываемое источнику, в нем не содержится, или содержится в другом контексте;
	
	\item неверная страница --- указан неправильный диапазон страниц, что затрудняет верификацию цитаты;
	
	\item отсутствует источник --- информация используется без ссылки на источник;
	
	\item призрачные цитаты --- источники цитируются по вторичным источникам без прямого обращения к первичному источнику.
	
\end{itemize}

Для решения задачи выявления этих ошибок используется автоматизированная верификация источников. Каждая проблема решается следующим образом:
\begin{itemize}
	\item некорректные источники выявляются через сравнение извлеченных метаданных с библиографическими базами;
	\item отсутствующая информация проверяется семантическим сопоставлением контекста цитирования с содержанием источника;
	\item неверные страницы исправляются через анализ структуры документа и извлечение точных диапазонов;
	\item отсутствующие источники обнаруживаются через поиск неподтвержденных утверждений;
	\item призрачные цитаты выявляются через анализ цепочек цитирования~\cite{cyberleninka_dissertacii, arxiv_auto_ie2024}.
\end{itemize}     

\subsection{Автоматический парсинг и извлечение метаданных}

Для проверки корректности источников необходимо автоматизированно извлекать метаданные из PDF и других документов. Системы парсинга, такие как CERMINE, GROBID и PDFDataExtractor, используют компьютерное зрение и обработку естественного языка для распознавания структуры документа и извлечения текста, авторов, названия, года публикации, DOI (Digital Object Identifier), диапазонов страниц, и списка литературы. Извлеченные метаданные затем сопоставляются с библиографическими базами данных~\cite{arxiv_auto_ie2024}.

\subsection{Проверка контекста цитирования и соответствия источнику}

Анализ контекста цитирования включает проверку того, действительно ли утверждение в цитирующем тексте соответствует содержимому исходного документа. Для решения этой задачи применяются методы NLP:

\begin{itemize}
	
	\item выделение синтаксических единиц вокруг ссылки (несколько предложений до и после цитирования);
	
	\item извлечение соответствующих фрагментов из исходного документа, соответствующих ключевым термам из цитирующего предложения;
	
	\item сравнение семантической схожести между контекстом ссылки и релевантными фрагментами источника с использованием косинусной меры близости;
	
	\item оценка соответствия на основе порога схожести для определения, является ли цитирование корректным~\cite{arxiv_citecheck2025, arxiv_auto_ie2024}.
	
\end{itemize}

\chapter{Экспериментальное исследование и оценка эффективности}

\section{Метрики оценки качества}

В качестве основных показателей качества работы алгоритмов используются полнота, точность и $F$-мера. Для оценки эффективности систем обнаружения используются следующие метрики:

\begin{itemize}
	
	\item точность --- доля верно обнаруженных случаев плагиата из всех случаев, помеченных как плагиат. Высокая точность означает низкий процент ложных срабатываний.
	
	\item полнота --- доля верно обнаруженных случаев из всех действительно существующих случаев плагиата. Высокая полнота означает, что система пропускает мало реальных случаев плагиата.
	
	\item $F$-мера --- гармоническое среднее точности и полноты, дающее единую метрику качества работы алгоритма;
	
\end{itemize}

\section{Учет порядка слов}


Помимо метрик полноты и точности, важно учитывать, как методы ведут себя с точки зрения порядка слов и вычислительной сложности. Методы на основе предложений (Long Sent, Heavy Sent) в значительной степени зависят от синтаксической структуры и потому лучше выявляют парафраз в пределах предложений, но чувствительны к сильной переразметке текста. Лексические сигнатуры (TF $\cdot$ IDF, TF $\cdot$ RIDF, Opt Freq, I-Match) и шингловые методы (мегашинглы, Log Shingles) реализуют модель <<мешка слов>> или локальных шинглов и менее чувствительны к порядку предложений, но зачастую хуже различают семантический парафраз. С точки зрения вычислительных затрат методы на основе шинглов и лексических сигнатур проще и масштабируемее, тогда как нейросетевые семантические модели и стилометрический анализ требуют значительно больших ресурсов, но потенциально обеспечивают лучшую чувствительность к сложным формам плагиата.

\section{Результаты сравнительного исследования}

Сравнение проводилось на коллекции русскоязычных веб-документов РОМИП путем поиска заимствований в текстах научных статей. Тексты включали как полные дубликаты, так и парафразы и частичные переиспользования. Система проверяла каждый алгоритм на единообразных наборах тестовых пар документов. Тексты включали как полные дубликаты, так и парафразы и частичные переиспользования. Коллекция содержала 1250 научных статей из различных областей (информатика, математика, физика) общим объемом более 5000 страниц. Система проверяла каждый алгоритм на единообразных наборах тестовых пар документов, включая пары с известным заимствованием и пары без заимствований для оценки ложных срабатываний.

\begin{table}[H]
	
	\centering
	
	\begin{tabular}{|l|c|c|c|c|}
		
		\hline
		
		\textbf{Алгоритм} & \textbf{Полнота} & \textbf{Точность} & \textbf{$F$-мера} \\
		
		\hline
		
		Long Sent & 0.84 & 0.80 & 0.82 \\
		
		\hline
		
		TF & 0.60 & 0.94 & 0.73 \\
		
		\hline
		
		Opt Freq & 0.59 & 0.94 & 0.73 \\
		
		\hline
		
		TF $\cdot$ RIDF & 0.59 & 0.95 & 0.73 \\
		
		\hline
		
		Heavy Sent & 0.62 & 0.86 & 0.72 \\
		
		\hline
		
		TF $\cdot$ IDF & 0.54 & 0.96 & 0.69 \\
		
		\hline
		
		Lex Rand & 0.50 & 0.97 & 0.66 \\
		
		\hline
		
		Descr Words & 0.44 & 0.77 & 0.56 \\
		
		\hline
		
		Log Shingles & 0.39 & 0.97 & 0.56 \\
		
		\hline
		
		Megashingles & 0.36 & 0.91 & 0.51 \\
		
		\hline
		
		MD5 & 0.23 & 1.00 & 0.38 \\
		
		\hline
		
	\end{tabular}
	
	\caption{Сравнение метрик качества алгоритмов обнаружения заимствований в порядке убывания рейтинга}
	
	\label{tab:algorithm_comparison}
	
\end{table}

Результаты показывают, что алгоритм выбора длинных предложений (Long Sent) показал наилучшие результаты по $F$-мере (0.82), сочетая высокую полноту (0.84) и точность (0.80). Успех метода объясняется тем, что длинные предложения содержат больше уникальной информации и реже подвергаются существенным модификациям при парафразировании, что делает их надежными индикаторами заимствования.

Лексические методы (TF, TF $\cdot$ IDF, TF $\cdot$ RIDF, Opt Freq) демонстрируют высокую точность (0.94--0.97), но умеренную полноту (0.54--0.60). Высокая точность обусловлена тем, что эти методы выявляют документы с существенным совпадением ключевых слов, что минимизирует ложные срабатывания. Однако низкая полнота указывает на чувствительность к парафразированию: при замене слов синонимами или изменении формулировок методы пропускают заимствования.

Алгоритм MD5 показывает идеальную точность (1.00), но крайне низкую полноту (0.23), что объясняется его способностью обнаруживать только точные побайтовые копии документов. Любое минимальное изменение текста приводит к полному изменению хеш-суммы.

Метод мегашинглов уступает более простым алгоритмам ($F$-мера 0.51), что объясняется строгими требованиями к совпадению супершинглов: для признания документов похожими необходимо совпадение хотя бы одного мегашингла из 15, что делает метод чувствительным к локальным модификациям текста.

Методы, основанные на шинглах и мегашинглах, хорошо подходят для масштабных систем, так как требуют минимальной памяти и времени, но их точность и полнота ниже, чем у методов на основе предложений~\cite{zelenkov2007}.

\section{Ограничения исследования}

Проведенное экспериментальное исследование имеет ряд ограничений, которые необходимо учитывать при интерпретации полученных результатов:

\begin{itemize}
	
	\item эксперимент проведен только на русскоязычных текстах научных статей, что ограничивает обобщаемость результатов на тексты других языков и стилей;
	
	\item коллекция ограничена научными статьями из областей информатики, математики и физики; методы могут демонстрировать иную эффективность на текстах других жанров (художественная литература, публицистика, техническая документация);
	
	\item не тестировались кросс-языковые заимствования и переводной плагиат, которые требуют специализированных подходов;
	
	\item размер коллекции (1250 документов) может быть недостаточен для полной оценки масштабируемости алгоритмов на больших массивах данных;
	
	\item не проводилось тестирование современных нейросетевых методов (BERT, Siamese networks) на той же коллекции, что ограничивает возможность прямого сравнения с классическими подходами.
	
\end{itemize}

