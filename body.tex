\chapter{Классификация и основные подходы к обнаружению заимствований}

\section{Классификация алгоритмов и методов детекции}

Все многообразие подходов к обнаружению некорректных заимствований можно систематизировать в пять основных классов:

\begin{enumerate}
	
	\item методы синтаксического сравнения и отпечатков текста;
	
	\item лексико-статистические методы и метрики схожести;
	
	\item методы семантического/смыслового сопоставления;
	
	\item модели авторского стиля и стилометрии;
	
	\item алгоритмы анализа и верификации цитирования.
	
\end{enumerate}

Такая классификация позволяет проанализировать каждый подход с точки зрения его особенностей, преимуществ и ограничений~\cite{zelenkov2007}.

\chapter{Синтаксические и лексико-статистические методы}

\section{Методы синтаксического сравнения и отпечатков текста}

\subsection{Общие принципы}

Идея отпечатков заключается в том, чтобы получить компактное представление текста, сохраняющее его уникальные черты, и сравнивать эти представления вместо полных текстов. Это позволяет значительно ускорить сравнение больших объемов текстовых данных. Основной принцип --- текст разбивается на перекрывающиеся или неперекрывающиеся фрагменты фиксированной длины, каждому фрагменту вычисляется хеш, и затем сравниваются хеши~\cite{zelenkov2007}.

\subsection{Шинглы и метод Бродера}

Одним из первых исследований в области нахождения нечетких дубликатов является работа А. Бродера, в которой был предложен синтаксический метод оценки сходства между документами, основанный на представлении документа в виде множества всевозможных последовательностей фиксированной длины $k$, состоящих из соседних слов. Такие последовательности были названы шинглами. Два документа считались похожими, если их множества шинглов существенно пересекались.

Поскольку число шинглов примерно равно длине документа в словах, были предложены два метода сэмплирования для получения репрезентативных подмножеств. Первый метод оставлял только те шинглы, чьи дактилограммы (вычисляемые по алгоритму Рабина-Карпа) делились без остатка на некоторое число $m$. Второй метод отбирал фиксированное число $s$ шинглов с наименьшими значениями дактилограмм~\cite{zelenkov2007}.

\subsection{Алгоритм Winnowing}

Winnowing --- это модификация простого $k$-граммного анализа, предложенная для повышения эффективности. Алгоритм работает следующим образом: текст разбивается на $k$-граммы (подстроки длины $k$); для каждой $k$-граммы вычисляется хеш-значение; из всех хешей выбираются только минимальные значения в пределах скользящего окна размером $w$. Эти избранные хеши образуют отпечаток документа, после чего отпечатки различных документов сравниваются для определения схожести. Преимущество Winnowing --- устойчивость к перестановкам фрагментов и синтаксическим изменениям, сохраняя низкую вычислительную сложность. Алгоритм используется в системах MOSS и JPlag для обнаружения плагиата в исходном коде и текстах~\cite{matec_winnowing2018}.

\subsection{Дактилограммы и алгоритм Рабина-Карпа}

Одними из первых исследований в области нахождения нечетких дубликатов являются работы U. Manber и N. Heintze. Дактилограмма файла или документа включает все текстовые подстроки фиксированной длины. Численное значение дактилограмм вычисляется с помощью алгоритма случайных полиномов Рабина-Карпа. В качестве меры сходства двух документов используется отношение числа общих подстрок к размеру файла или документа. Алгоритм Рабина-Карпа основан на быстром вычислении хешей для перекрывающихся подстрок с использованием скользящего окна. Полиномиальное хеширование позволяет за $O(1)$ пересчитать хеш для следующей подстроки на основе хеша предыдущей. Метод особенно эффективен при поиске точных совпадений в больших массивах текста~\cite{zelenkov2007}.

\subsection{Мегашинглы и супершинглы}

Дальнейшим развитием концепций Бродера являются исследования D. Fetterly. Для каждого документа вычисляются 84 дактилограммы по алгоритму Рабина-Карпа с помощью взаимно-однозначных и независимых функций. В результате каждый документ представлялся 84 шинглами, минимизирующими значение соответствующей функции. Затем 84 шингла разбиваются на 6 групп по 14 шинглов в каждой. Эти группы называются супершинглами. Документ представляется всевозможными попарными сочетаниями из 6 супершинглов, которые называются мегашинглами. Число таких мегашинглов равно 15. Два документа сходны по содержанию, если у них совпадает хотя бы один мегашингл. Ключевое преимущество данного алгоритма состоит в том, что любой документ (в том числе и очень маленький) всегда представляется вектором фиксированной длины, и сходство определяется простым сравнением координат вектора~\cite{zelenkov2007}.

\subsection{N-граммный анализ и SimHash}

$N$-граммный анализ --- одна из самых базовых, но действенных методик. Документ представляется как набор $n$-грамм (последовательности из $n$ символов или слов). Сравнение документов производится по пересечению их $n$-грамм. SimHash расширяет эту идею: для каждого документа строится битовый отпечаток фиксированной длины путем комбинирования хешей его $n$-грамм. Два документа считаются схожими, если расстояние Хемминга между их отпечатками мало~\cite{mdpi_fingerprint2021}.

\subsection{Locality Sensitive Hashing (LSH)}

LSH --- это техника, которая хеширует входные элементы таким образом, что похожие элементы с высокой вероятностью получают один и тот же хеш. Для поиска похожих текстов LSH позволяет за логарифмическое время находить кандидатов без полного сравнения со всеми документами в базе. Это особенно ценно для масштабируемых систем~\cite{mdpi_fingerprint2021}.

\section{Лексико-статистические методы и метрики схожести}

\subsection{TF-IDF и векторное пространство}

TF-IDF (Term Frequency - Inverse Document Frequency) --- классический метод, преобразующий тексты в векторное пространство. Каждому слову в документе присваивается вес, учитывающий его частоту в документе (TF) и редкость в коллекции (IDF). Строится частотный словарь документа, упорядоченный по убыванию произведения TF*IDF. Затем выбираются и сцепляются в алфавитном порядке топ-N слов с наибольшими весами. В качестве сигнатуры документа вычисляется контрольная сумма (например, CRC32 или MD5) полученной строки. Два текста сравниваются как векторы, и схожесть оценивается с помощью косинусной меры. Документы с высокой косинусной схожестью считаются потенциально плагиатными~\cite{tfidf_hybrid2022}.

\subsection{Метод I-Match}

Сигнатурный подход, основанный на лексических принципах, был предложен A. Chowdhury. Основная идея состоит в вычислении дактилограммы I-Match для представления содержания документов. Сначала для исходной коллекции документов строится словарь $L$, который включает слова со средними значениями IDF. Слова с большими и маленькими значениями IDF отбрасываются. Для каждого документа формируется множество $U$ различных слов, входящих в него, и определяется пересечение $U$ и словаря $L$. Список слов, входящих в пересечение, упорядочивается, и для него вычисляется I-Match сигнатура (хеш-функция SHA1). Два документа считаются похожими, если у них совпадают I-Match сигнатуры~\cite{zelenkov2007}.

\subsection{Метод опорных слов}

Метод опорных слов, предложенный С. Ильинским, заключается в следующем. Сначала из индекса по определенному правилу выбирается множество из $N$ слов, называемых опорными. Затем каждый документ представляется $N$-мерным двоичным вектором, где $i$-я координата равна 1, если $i$-е опорное слово имеет в документе относительную частоту выше определенного порога, и равна 0 в противном случае. Этот двоичный вектор называется сигнатурой документа. Два документа похожи, если у них совпадают сигнатуры. Для каждого слова строится распределение документов по внутридокументной частоте. Проводится несколько итераций оптимизации, в которых максимизируется покрытие документов при фиксированной точности, а затем максимизируется точность при фиксированном покрытии~\cite{zelenkov2007}.

\subsection{Расстояние Левенштейна}

Расстояние Левенштейна (редакционное расстояние) --- это минимальное количество однозначных операций редактирования (вставка, удаление, замена символа), необходимых для преобразования одной строки в другую. Метрика полезна для поиска близких вариаций текста, включая опечатки и синтаксические ошибки. Однако на больших текстах требует квадратичного времени, что ограничивает его применимость в масштабных системах~\cite{mit_levenshtein2020}.

\subsection{Методы на основе предложений}

Исследование Зеленкова и Сегаловича включает алгоритмы, основанные на выборе характерных предложений документа. Long Sent: выбираются 2 самых длинных предложения документа, сцепляются в алфавитном порядке, и вычисляется контрольная сумма CRC32. Этот алгоритм показал наивысшую $F$-меру (0.82) среди всех исследованных методов. Heavy Sent: вычисляется вес каждого предложения как сумма произведений TF*IDF для всех слов предложения. Выбираются 2 самых тяжелых предложения, и для них вычисляется сигнатура~\cite{zelenkov2007}.

\chapter{Семантические методы и стилометрический анализ}

\section{Методы семантического сопоставления}

\subsection{Встраивания слов: Word2Vec, fastText, GloVe}

Встраивания слов преобразуют слова в плотные векторы в пространстве низкой размерности, где семантически похожие слова имеют близкие представления. Word2Vec использует модель Skip-gram или CBOW для обучения на больших текстовых корпусах. fastText расширяет Word2Vec, учитывая информацию о подсловах (символьные n-граммы), что помогает справляться с редкими словами и опечатками~\cite{arxiv_bertembed2020}.

\subsection{BERT и трансформерные модели}

BERT (Bidirectional Encoder Representations from Transformers) --- модель глубокого обучения, предварительно обученная на большом количестве текста, которая генерирует контекстные представления слов и предложений. В отличие от статических встраиваний, BERT учитывает контекст слова в предложении, что позволяет более точно захватывать смысл. Sentence-BERT (SBERT) расширяет BERT для создания встраиваний всех предложений, которые прямо оптимизированы для семантической схожести~\cite{aclan_phrase_bert2021}.

\subsection{Siamese и Triplet Loss архитектуры}

Siamese network состоит из двух или более копий одной и той же нейронной сети, которые обрабатывают два входа и генерируют представления, сравниваемые для определения схожести. Triplet loss минимизирует расстояние между якорным примером и похожим примером, одновременно максимизируя расстояние между якорем и непохожим примером. Эти архитектуры эффективны для обучения моделей, которые захватывают метрику сходства~\cite{pmc_siamese2022}.

\subsection{LSTM и RNN с механизмом внимания}

Recurrent Neural Networks (RNN), особенно в форме LSTM (Long Short-Term Memory) или GRU (Gated Recurrent Unit), могут обрабатывать последовательности и захватывать долгосрочные зависимости в тексте. Добавление attention механизма позволяет модели сосредоточиться на наиболее релевантных частях входа при сравнении двух текстов. BiLSTM (bidirectional LSTM) обрабатывает текст в обоих направлениях, улучшая представление~\cite{arxiv_lstm_plag2021}.

\section{Модели авторского стиля и стилометрия}

\subsection{Основы стилометрического анализа}

Стилометрия --- это область, изучающая характеристики письменного стиля, которые отличают одного автора от другого. Предполагается, что у каждого автора есть уникальный стиль, который сохраняется даже при осознанной попытке его изменить. Признаки стилометрии включают:
\begin{itemize}
	\item среднюю длину предложения, распределение длин предложений; \item часто используемые функциональные слова (предлоги, союзы, артикли);
	\item частотность части речи (POS tags);
	\item лексическое разнообразие (type-token ratio);
	\item использование пунктуации и капитализации;
	\item лексическую плотность;
	\item читаемость текста~\cite{acl_intrinsic_2015}.
\end{itemize}

\subsection{Интринсивная детекция плагиата}

Интринсивная (intrinsic) детекция плагиата ищет признаки копирования внутри документа, без привлечения внешних источников. Основной метод --- поиск стилистических разрывов: фрагменты заимствованного текста обычно имеют стиль отличающийся от основного стиля документа. Путем анализа последовательных блоков текста можно выявить участки, где стиль резко меняется~\cite{acl_intrinsic_2015}.

\section{Анализ и верификация цитирования}

\subsection{Проблемы и типы ошибок}

Ошибки в цитировании принимают различные формы~\cite{cyberleninka_dissertacii}:

\begin{itemize}
	\item некорректный источник --- ссылка приведена неправильно или указывает на совершенно иное произведение;
	\item отсутствующая информация в источнике --- утверждение, приписываемое источнику, в нем не содержится;
	\item неверная страница --- указан неправильный диапазон страниц;
	\item отсутствует источник --- информация используется без ссылки на источник.
\end{itemize}

\subsection{Автоматическая парсинг и извлечение метаданных}

Для проверки корректности источников необходимо автоматизированно извлекать метаданные из PDF и других документов. Системы, такие как CERMINE, GROBID и PDFDataExtractor, используют компьютерное зрение и обработку естественного языка для распознавания структуры документа и извлечения текста, авторов, названия, года публикации, DOI, страниц, и списка литературы~\cite{arxiv_auto_ie2024}.

\subsection{Анализ контекста цитирования}

Анализ контекста включает проверку того, действительно ли утверждение в цитирующем тексте соответствует содержимому исходного документа. Для этого применяются методы NLP:
\begin{itemize}
	\item выделение синтаксических единиц вокруг ссылки (несколько предложений до и после);
	\item извлечение соответствующих фрагментов из исходного документа; \item сравнение семантической схожести между контекстом ссылки и релевантными фрагментами источника;
	\item оценка соответствия на основе порога схожести~\cite{arxiv_citecheck2025}.
\end{itemize}

\chapter{Экспериментальное исследование и оценка эффективности}

\section{Метрики оценки качества}

В качестве основных показателей качества работы алгоритмов используются полнота, точность и $F$-мера. Для оценки эффективности систем обнаружения используются следующие метрики~\cite{zelenkov2007}:

\begin{itemize}
	\item точность --- доля верно обнаруженных случаев плагиата из всех случаев, помеченных как плагиат;
	\item полнота --- доля верно обнаруженных случаев из всех действительно существующих случаев плагиата;
	\item $F$-мера --- гармоническое среднее точности и полноты;
\end{itemize}

\section{Результаты сравнительного исследования}

\begin{table}[H]
	\centering
	\begin{tabular}{|l|c|c|c|c|}
		\hline
		\textbf{Алгоритм} & \textbf{Полнота} & \textbf{Точность} & \textbf{$F$-мера} \\
		\hline
		Long Sent & 0.84 & 0.80 & 0.82 \\
		\hline
		TF & 0.60 & 0.94 & 0.73 \\
		\hline
		Opt Freq & 0.59 & 0.94 & 0.73 \\
		\hline
		TF-RIDF & 0.59 & 0.95 & 0.73 \\
		\hline
		Heavy Sent & 0.62 & 0.86 & 0.72 \\
		\hline
		TF-IDF & 0.54 & 0.96 & 0.69 \\
		\hline
		Lex Rand & 0.50 & 0.97 & 0.66 \\
		\hline
		Descr Words & 0.44 & 0.77 & 0.56 \\
		\hline
		Log Shingles & 0.39 & 0.97 & 0.56 \\
		\hline
		Megashingles & 0.36 & 0.91 & 0.51 \\
		\hline
		MD5 & 0.23 & 1.00 & 0.38 \\
		\hline
	\end{tabular}
	\caption{Сравнение метрик качества алгоритмов обнаружения заимствований в порядке убывания рейтинга}
	\label{tab:algorithm_comparison}
\end{table}

Результаты показывают, что алгоритм выбора длинных предложений (Long Sent) показал наилучшие результаты по $F$-мере, сочетая высокую полноту и точность. Лексические методы (TF, TF*IDF, Opt Freq) показывают высокую точность, но умеренную полноту. Метод мегашинглов уступает более простым алгоритмам, что объясняется строгими требованиями к совпадению супершинглов~\cite{zelenkov2007}.
